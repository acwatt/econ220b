---
title: "Econ 220B PS 2"
author: "AC Watt"
email: aaron@acwatt.net
date: "2021-12-03"
output:
  pdf_document:
    toc: true
    toc_depth: 2
header-includes:
   - \usepackage{dcolumn}
   - \usepackage{amsmath}
   - \usepackage{floatrow}
   - \floatsetup[figure]{capposition=top}
   - \usepackage{float}
   - \floatstyle{plaintop}
   - \restylefloat{table}
knit: (function(inputFile, encoding) { rmarkdown::render(inputFile, encoding = encoding, output_file = file.path(dirname(inputFile), 'Econ_220b_PS2_acWatt_2021.pdf')) })
---

```{r setup, message=F, echo=F}
rm(list=ls())
knitr::opts_chunk$set(echo = F)
```
<!--
R version 3.6.3 (2020-02-29)
Purpose of script: run code necessary for ps2b for ARE 213

Notes: Need to open ps2b.Rmd from the ps2b folder (without Rstudio being started first)
if Rstudio is already started, the working directory will not be set
to the ps2b/ folder

\usepackage{dcolumn}: dcolumn is needed in latex compilation for multicolumn tables
-->

```{r Settings}
# stargazer table type (html, latex, or text)
# Change to latex when outputting to PDF, html when outputting to html
table_type = "latex"
Cache = TRUE

```




```{r packages, message=F, warning=F}
# Packages
library(stargazer)
library(ggplot2)
library(readxl)
library(tidyverse)
library(lubridate)
library(kableExtra)
library(docstring)
```



\newpage
<!--=========================================================================-->
# Part a
<!--=========================================================================-->
**Construct sample statistics similar to Tabel IIa and IIb on page 1003.**
\vspace{1em}


```{r Table IIa, eval=T, cache=Cache, message=F, warning=F, results='asis'}
# Load top 11 rows only to get replacement dates and mileage
f = 'rust_data_clean.xlsx'
headers = read_excel(f, sheet='header_names', col_names='header')
sheets = excel_sheets(f)[1:8]

get_headers = function(f, sheets, i) {
    # Return dataframe of transformed i'th sheet
    read_excel(f, sheet=sheets[i], n_max=11, col_names=F, col_types="numeric") %>%
    t() %>%
    data.frame(stringsAsFactors = F) %>%
    mutate(sheet = sheets[i], 
           group = i)
}

# Initialize dataframe
replacement_dates = get_headers(f, sheets, 1)

# Fill dataframe with other sheets
for (i in 2:8) {
    df_ = get_headers(f, sheets, i)
    replacement_dates = rbind(replacement_dates, df_)
}

# Pivot dataframe and create variables
replacement_dates = replacement_dates %>%
    rename(!!set_names(paste0("X", 1:11), str_replace_all(headers$header, " ", "_"))) %>%
    mutate(purchase_date = ymd(str_c(purchase_year, purchase_month, "-01")),
           replace_date1 = ymd(str_c(engine_replacement_1_year, engine_replacement_1_month, "-01")),
           replace_date2 = ymd(str_c(engine_replacement_2_year, engine_replacement_2_month, "-01")),
           pivot_months_to_replace1 = interval(purchase_date, replace_date1) %/% months(1) + 1,
           pivot_months_to_replace2 = interval(replace_date1, replace_date2) %/% months(1) + 1,
           pivot_replace_mileage1 = ifelse(engine_replacement_1_odometer_reading == 0, NA,
                                           engine_replacement_1_odometer_reading),
           pivot_replace_mileage2 = ifelse(engine_replacement_2_odometer_reading == 0, NA, 
                                           engine_replacement_2_odometer_reading - engine_replacement_1_odometer_reading),
           initial_odometer_reading_date = ymd(str_c(initial_odometer_reading_year, initial_odometer_reading_month, "-01"))) %>%
    select(bus_number, purchase_date, replace_date1, pivot_months_to_replace1, replace_date2, pivot_months_to_replace2, everything())

# Make Table IIa
replacement_dates %>%
    pivot_longer(starts_with("pivot_"),
                 names_to = c(".value", "replacement"),
                 names_pattern = "pivot_(.*)(.)",
                 values_drop_na = T
    ) %>%
    mutate(group = as.character(group)) %>%
    bind_rows(mutate(., group = "Full Sample")) %>%
    group_by(group) %>%
    summarize(max_mileage = max(replace_mileage, na.rm=T),
              min_mileage = min(replace_mileage, na.rm=T),
              mean_mileage = as.integer(mean(replace_mileage, na.rm=T)),
              sd_mileage = as.integer(sd(replace_mileage, na.rm=T)),
              max_months = max(months_to_replace, na.rm=T),
              min_months = min(months_to_replace, na.rm=T),
              mean_months = mean(months_to_replace, na.rm=T),
              sd_months = sd(months_to_replace, na.rm=T),
              n = n()) %>% 
    complete(group = c(as.character(1:8), "Full Sample")) %>%
    mutate(across(everything(), ~replace_na(.x, 0))) %>%
    kbl(caption = "Summary of Replacement Data (Subsample of buses for which at least 1 replacement occurred)",
        col.names = c('Bus Group', 'Max', 'Min', 'Mean', 'Standard Deviation',
                      'Max', 'Min', 'Mean', 'Standard Deviation', 'Number of Observations'),
        align = 'cc', digits = 1, booktabs = T, format.args = list(big.mark = ",")
    ) %>%
    kable_styling(latex_options = "HOLD_position") %>%
    column_spec(1, width = "1.5cm") %>%
    column_spec(c(2:4, 6:8), width = "1cm") %>%
    column_spec(c(5, 9), width = "1.5cm") %>%
    column_spec(10, width = "2cm") %>%
    add_header_above(c(" " = 1, "Mileage at Replacement" = 4, "Elapsed Time (Months)" = 4, " " = 1)) %>%
    row_spec(8, hline_after = T)
    
```



```{r Table IIb, eval=T, cache=Cache, message=F, warning=F, results='asis'}
# Load rest of data below row 11 to get mileage readings
f = 'rust_data_clean.xlsx'
sheets = excel_sheets(f)[1:8]

get_data = function(f, sheet_, replacement_dates) {
    # Return transformed dataframe from sheet
    read_excel(f, sheet=sheet_, skip=11,
               col_names=paste0('bus_', filter(replacement_dates, sheet==sheet_)$bus_number), 
               col_types="numeric") %>%
        mutate(sheet = sheet_, 
               m = row_number()) %>%
        pivot_longer(starts_with("bus_"),
                     names_to = "bus_number",
                     names_prefix = 'bus_') %>%
        mutate(bus_number = as.numeric(bus_number),
               date = filter(replacement_dates, sheet==sheet_, bus_number==bus_number)$initial_odometer_reading_date %m+% months(m-1) ) %>%
        rename(c('mileage' = 'value'))
}

# Initialize dataframe
data = get_data(f, sheets[1], replacement_dates)

# Get rest of sheets
for (i in 2:8) {
    df_ = get_data(f, sheets[i], replacement_dates)
    data = rbind(data, df_)
}


# Merge actual replacement mileage from data at replacement date (not states odometer reading at replacement)
replacement_dates = replacement_dates %>%
    left_join(data %>% select(-m),
              by = c('bus_number'='bus_number', 'sheet' = 'sheet', 'replace_date1' = 'date')) %>%
    rename(mileage_replace1 = mileage) %>%
    left_join(data %>% select(-m),
              by = c('bus_number'='bus_number', 'sheet' = 'sheet', 'replace_date2' = 'date')) %>%
    rename(mileage_replace2 = mileage)

# Join on variables from headers and create new variables
data = data %>%
    arrange(sheet, bus_number) %>%
    left_join(replacement_dates %>% select(bus_number,
                                           initial_odometer_reading_date, 
                                           replace_date1, 
                                           replace_date2,
                                           mileage_replace1,
                                           mileage_replace2,
                                           group),
              by = 'bus_number') %>%
    mutate(date2 = initial_odometer_reading_date + months(m - 1),
           group = as.character(group),
           replaced = ifelse((date == replace_date1 & !is.na(replace_date1)) | (date == replace_date2 & !is.na(replace_date2)), 1, 0),
           x = case_when(date >= replace_date2 ~ mileage - mileage_replace2,
                         date >= replace_date1 ~ mileage - mileage_replace1,
                         TRUE ~ mileage))

# Make Table IIa
data %>%
    filter(is.na(replace_date1)) %>%
    arrange(date) %>%
    group_by(group) %>%
    filter(date == last(date), .by_group = TRUE) %>%
    bind_rows(mutate(., group = "Full Sample")) %>%
    summarize(max_mileage = max(mileage, na.rm=T),
              min_mileage = min(mileage, na.rm=T),
              mean_mileage = as.integer(mean(mileage, na.rm=T)),
              sd_mileage = as.integer(sd(mileage, na.rm=T)),
              max_months = max(m, na.rm=T),
              min_months = min(m, na.rm=T),
              mean_months = mean(m, na.rm=T),
              sd_months = sd(m, na.rm=T),
              n = n()) %>% 
    complete(group = c(as.character(1:8), "Full Sample")) %>%
    mutate(across(everything(), ~replace_na(.x, 0))) %>%
    kbl(caption = "Summary of Censored Data (subsample of buses for which no replacements occurred)",
        col.names = c('Bus Group', 'Max', 'Min', 'Mean', 'Standard Deviation',
                      'Max', 'Min', 'Mean', 'Standard Deviation', 'Number of Observations'),
        align = 'cc', digits = 1, booktabs = T, format.args = list(big.mark = ",")
    ) %>%
    kable_styling(latex_options = "HOLD_position") %>%
    column_spec(1, width = "1.5cm") %>%
    column_spec(c(2:4, 6:8), width = "1cm") %>%
    column_spec(c(5, 9), width = "1.5cm") %>%
    column_spec(10, width = "2cm") %>%
    add_header_above(c(" " = 1, "Mileage at May 1, 1985" = 4, "Elapsed Time (Months)" = 4, " " = 1)) %>%
    row_spec(8, hline_after = T)
    
```





\newpage
<!--=========================================================================-->
# Part b
<!--=========================================================================-->
**Estimate his model using a two-step procedure. Please briefly document your
estimation procedure in words. Create a table similar to the first five columns
of Table IX on page 1021. If you forward simulate the value function, see
Pakes Pollard (1989) for reference on how to draw errors and compute confidence
intervals.**

















\newpage
<!--=========================================================================-->
# Part c
<!--=========================================================================-->
**Estimate his model using a nested fixed point procedure. Create a table
similar to the first five columns of Table IX on page 1021.**

Steps:

1. Break the mileage into 5,000 mile bins

2. Create g-bins of 0, 1, and 2 for mileage in the intervals [0, 5000), [5000, 10,000) and [10,000, +$\infty$).


```{r Discretize the mileage}
# Create 5,000 mile bins
data = data %>%
    mutate(x5000 = cut(x,
                       breaks = seq(0, max(data$x)+5000, by=5000),
                       labels = F,
                       include.lowest = T)) %>%
    # Create g = number of x bins increased from previous period
    group_by(bus_number) %>%
    mutate(g = c(0,diff(x5000)),
           g = ifelse(g < 0, 0, g))

```

3. Define the cost functions to be estimated



4.




5. Estimate $\theta_3$ using Maximum Likelihood 
Does the mileage stay in the current bin, or move up to the next bin, conditional
on no replacement?

Assuming the buses within a group are independent and come from the same discrete 
transition probability distribution, then the likelihood function is just the
product of the likelihood functions for all the buses in the group, which are in 
turn, the product of the probabilities of observing the transitions in the data.

For group $g$ with $n_g$ buses and $T_g$ periods in it, the joint likelihood function $\ell^1_g$ is
\[
\ell^1_g(x_{1,1}, ..., x_{T_g,1}, ..., x_{T_g,n_g}, i_{1,1}, ..., i_{T, n_g}|x_{0,1}, ..., x_{0,n_g}, i_{0,1}, ..., i_{0, n_g}, \theta)
= \prod\limits_{k=1}^{n_g}\prod\limits_{t=1}^{T_g} p(x_{t,k} | x_{t-1,k}, i_{t-1,k}, \theta_3)
\]

```{r}
library(stats4)

t1 = 0.34
t2 = 0.33
t3 = 0.33
theta3.start = c(t1, t2, t3)
data_temp = data %>%
    # Remove the initial month, and the months when replacements happened
    filter(m != 1, replaced != 1) %>%
    arrange(date) %>%
    group_by(group, bus_number) %>%
    distinct(mileage, .keep_all = T) %>%
    ungroup() %>%
    arrange(group, bus_number, date)


single_llh = function(theta3, g_vec) {
    #' Return product of all the transition probabilities
    #' @param theta3 vector of 3 transition probabilities
    #' @param g_vec vector of g values, how many x-bins did this bus advance this
    #' period? between 0 and 2 for our sample

    # convert g_vec to indicies of the theta3 vector (theta3[1] = probability of moving zero x-bins)
    index = as.vector(g_vec) + 1
    # Estimate joint probability conditional on theta3 (product of each bus
    # event of advancing the specified number of bins)
    s = 0
    for (idx in index) {
        s = s + log(theta3[idx])
    }
    return(s)
}


minus_joint_llh = function(theta3, df) {
    #' Return product of all transition probabilities for all buses in group

    # If any theta is negative, return NA to prevent using this as solution
    # if (min(theta3) < 0) return(Inf)
    # if (max(theta3) > 1) return(Inf)
    # if (sum(theta3) > 1) return(Inf)
    
    # Make sure theta3 items sum to 1
    # theta3 = theta3 / sum(theta3)
    # Iterate through buses, get log likelihood for each bus, add up all bus LL's
    buses = unique(df$bus_number)
    nLL = 0
    for (bus in buses) {
        gvec = filter(df, bus_number == bus)$g
        nLL = nLL - single_llh(theta3, gvec) # changing to negative LL using (-)
    }
    penalty1 = (sum(theta3) - 1)^2 * 1e6
    penalty2 = ifelse(max(theta3) > 1, (max(theta3) - 1)^2 * 1e8, 0)
    penalty3 = ifelse(min(theta3) < 0, (min(theta3) - 0)^2 * 1e8, 0)
    # print(nLL + penalty1 + penalty2 + penalty3)
    return(nLL + penalty1 + penalty2 + penalty3) # + penalty1 + penalty2 + penalty3
}

# data_g = data_temp %>% filter(group == 8)
# out1 = nlm(minus_joint_llh, theta3.start, df = data_g, print.level = 1)
# sum(out1$estimate)


# use nlm() to minimize -log(likelihood(param))
theta3_df = data.frame(theta31=double(), theta32=double(), theta33=double(), group=integer())
for (group_ in 1:8) {
    data_g = data_temp %>% filter(group == group_)
    out = nlm(minus_joint_llh, theta3.start, df = data_g, print.level = 1)
    est = out$estimate / sum(out$estimate)
    df_ = data.frame(theta31 = est[1], theta32 = est[2], theta33 = est[3], group=group_)
    theta3_df = rbind(theta3_df, df_)
}


```

```{r other optimization functions with constraints, eval=F}
# optim
out2 = optim(theta3.start, minus_joint_llh, method = "L-BFGS-B", df = data_g, lower = c(0,0,0), upper = c(1,1,1))
sum(out2$par)


# nloptr
library('nloptr')
# iterate over groups (temp group = 8)
data_g = data_temp %>% filter(group == 8)
# Objective Function
minimize_f = function(theta3) minus_joint_llh(theta3, data_g)

# Equality constraints
eval_g_eq = function(theta3) theta3[1] + theta3[2] + theta3[3] - 1

# Set optimization options.
local_opts <- list( "algorithm" = "NLOPT_LD_MMA", "xtol_rel" = 1.0e-6 )
opts <- list( "algorithm"= "NLOPT_LD_AUGLAG",
              "xtol_rel"= 1.0e-6,
              "maxeval"= 1000,
              "local_opts" = local_opts,
              "print_level" = 0 )

# Optimize
nloptr(x0 = theta3.start,
       eval_f = minimize_f,
       lb = c(0,0,0),
       ub = c(1,1,1),
       eval_g_eq = eval_g_eq,
       opts = opts
)



# mle
data_temp %>%
    filter(group == 1) %>%
    '$'(g) %>%
    tabulate()

    fit1 = mle(minus_joint_llh, theta3 = theta3.start,
         method = "L-BFGS-B", lower = c(0,0,0), upper = c(1,1,1),
         fixed = list(df = data_g))

nLL_wrapper = function(theta3 = theta3.start) {
    return(data %>%
        arrange(date) %>%
        group_by(group, bus_number) %>%
        distinct(mileage, .keep_all = T) %>%
        ungroup() %>%
        arrange(group, bus_number, date) %>%
        minus_joint_llh(theta3, data = ., group = 1)
    )
}

mle(nLL_wrapper, start = list(theta3 = theta3.start))


```



```{r cost functions}
# Need to normalize all cost functions so c(0, theta1) = 0 (pg. 1015)
# Polynomial
c1 = function(xvec, theta1) sapply(xvec, function(x) theta1[1]*x + theta1[2]*x^2 + theta1[3]*x^3)
# exponential
c2 = function(xvec, theta1) sapply(xvec, function(x) theta1[1]*( exp(theta1[2]*x) - 1 ))
# hyperbolic
c3 = function(xvec, theta1) sapply(xvec, function(x) theta1[1]/(91-x) - theta1[1]/(91))
# square root
c3 = function(xvec, theta1) sapply(xvec, function(x) theta1[1]*x^0.5)

```

3.b Define utility function

```{r utility function}
U = function(x, i, cost_fun, theta1, RC) {
    #' Returns utility
    #' 
    #' @param cost_fun function, returning cost, with inputs x, theta1
    #' @param theta1 vector, matching the necessary dimensionality for cost_fun
    #' @param d integer, 0 or 1, represents no engine replacement or replacement
    #' @param RC float, expected cost of engine replacement
    #' @param e1 float, unobserved replacement cost shock
    #' @param e0 float, unobserved continuation (reported engine quality) shock
    ufun = function(x, i) {
        if (i == 1) { # replaced the engine
            return( -RC - cost_fun(0, theta1))
        }
        if (i == 0) { # did not replace the engine
            return( -cost_fun(x, theta1))
        }
    }
    return(ufun)
}
```


```{r Define EV}
N = 90

theta3.fun = function(group_, theta3_df) {
    #' Return theta3 vector for group_ from theta3 dataframe
    df = theta3_df %>% filter(group==group_)
    theta3 = c(df$theta31, df$theta32, df$theta33)
    return(theta3)
}

P.fun = function(theta3) {
    #' Return an N x N markov transition matrix from theta3 probabilities
    Pmat <- matrix(0, nrow = N, ncol = N)
    for(col in 1:N){
        for (row in 1:N) {
            if (row == col & row < N) {  # Fill in the diagonal with theta31
                Pmat[row, col] = theta3[1]
            }
            if (row == col-1 & row < N-1) {  # Fill in the upper off-diagonal with theta32
                Pmat[row, col] = theta3[2]
            }
            if (row == col-2 & row < N-1) {  # Fill in the upper off-diagonal with theta32
                Pmat[row, col] = theta3[3]
            }
        }
    }
    # Some manual entry
    Pmat[N, N] = 1
    Pmat[N-1, N] = 1 - theta3[2]
    return(Pmat)
}

g.fun = function(y, theta3) {
    #' Return theta3 probability for value y (# of bins jumped this period)
    if (y==0) return(theta3[1])
    if (y==1) return(theta3[2])
    if (y==2) return(theta3[3])
    else print('WRONG VALUE OF y')
}

T.EV = function(P, c, beta, EV, RC) {
    #' Return new nx1 EV vector
    #' @param P nxn markov transition matrix (no engine replacement)
    #' @param c nx1 cost vector, given theta1
    #' @param beta discount factor
    #' @param EV old EV vector
    #' @param RC engine replacement cost
    return(
        P %*% log(
            exp( -c + beta*EV ) +
            exp( -RC - c[1] + beta*EV[1] )
        )
    )
}

pk.fun = function(c, beta, EV, RC) {
    #' Return new nx1 values for P(0|x, theta)
    #' @param c nx1 cost vector, given theta1
    #' @param beta discount factor
    #' @param EV old EV vector
    #' @param RC engine replacement cost
    return(
        1/(
            1 + exp(c - beta*EV - RC - c[1] + beta*EV[1])
        )
    )
}


EV.sequential = function(theta1, theta3, c, RC, P, beta,
                  iteration_max = 10000, verbose = F,
                  accuracy_threshold = 1e-6) {
    #' Return sequential approximation EV given theta1, theta3, cost function, and RC
    #' @param theta1 kx1 vector of theta1 parameters, k=# of params of cost_fun
    #' @param theta3 3x1 vector of theta3 transition probabilities
    #' @param cost_fun cost function that takes scalar x and theta1 kx1 vector
    #' @param RC scalar, engine replacement cost
    #' @param iteration_max integer, max number of iterations for approximating EV
    #' @param verbose bool, TRUE means print more information during iterations
    #' @param accuracy_threshold float, small number to get below when iterating
    #'      toward EV fixed point

    x = 1:N;  EVold = rep(0, N);
    start_t = proc.time()
    # norms = c()
    
    # Iterate toward fixed point EV
    for (k in 1:iteration_max) {
        # Get new EV
        EVnew = T.EV(P, c, beta, EVold, RC)
        # Calculate distance metric
        diff = abs(EVold - EVnew)
        sup_norm = max(diff)
        # norms[k] = sup_norm
        if (k%%100 == 0 & verbose) print(paste('iteration', k, ' norm:', sup_norm))
        if (sup_norm < accuracy_threshold) {
            if (verbose) {
                print('WITHIN TOLERANCE! EV function approximated.')
                print(paste('Total seconds:', proc.time()[3] - start_t[3]))
                print(paste('Average seconds per iteration:', 
                            (proc.time()[3] - start_t[3]) / iteration_max ))
            }
            return(list(EVnew=EVnew, EVold=EVold))
        }
        EVold = EVnew
    }
    print(paste('MAX ITERATIONS REACHED! EV function approximated to', sup_norm))
    print(paste('Total seconds:', proc.time()[3] - start_t[3]))
    print(paste('Average seconds per iteration:', 
                (proc.time()[3] - start_t[3]) / iteration_max ))
    return(list(EVnew=EVnew, EVold=EVold))
    # Plot norm distance vs iterations, log-log scale
    # data.frame(y=norms) %>%
    #     mutate(x=row_number()) %>%
    #     ggplot() + geom_line(aes(x,y)) +
    #     xlab('iterations (log10 scale)') + ylab('Sup Norm (log10 scale)') +
    #     ggtitle('Sup Norm between EV and EV Sequential update') +
    #     scale_y_continuous(trans='log10') + scale_x_continuous(trans='log10')
}


Tprime.fun = function(beta, P, pk) {
    return(beta*(cbind(
        1 - rowSums( P[,2:N] * pk[2:N] ),  # firstcol
        P[,2:N] * pk[2:N]  # n-1 right cols
    )))
}


EV.nk = function(EVold, Tprime, c, dc, RC, P, beta,
                 iteration_max = 1000, verbose = F,
                 accuracy_threshold = 1e-10, print_every=100) {
    #' Return Newton-Kantorovich approximated EV
    x = 1:N
    start_t = proc.time()
    # Iterate toward fixed point EV
    for (k in 1:iteration_max) {
        # Get new EV
        T.EV(P, c, beta, EVold, RC)
        EVnew = EVold - solve(diag(N) - Tprime) %*% (EVold - T.EV(P, c, beta, EVold, RC))
        # Calculate distance metric
        sup_norm = max(abs(EVold - EVnew))
        if (k%%print_every == 0 & verbose) print(paste('iteration', k, ' norm:', sup_norm))
        if (sup_norm < accuracy_threshold) {
            if (verbose) {
                print(paste('WITHIN TOLERANCE! EV function approximated.', k, 'iterations'))
                print(paste('Total seconds:', proc.time()[3] - start_t[3]))
                print(paste('Average seconds per iteration:', 
                            (proc.time()[3] - start_t[3]) / iteration_max ))
            }
            
            # Recenter the value function (max over x)
            K = max(-c + beta*EVnew)
            
            # Derivative of T w.r.t. beta
            dTdb = (-beta * (1-beta) * (EVnew[1] + P %*% (EVnew - EVnew[1]) * pk))
            
            # Derivative of T w.r.t. RC
            dTdRC = P %*% pk - 1
            # browser()
            # Derivative of T w.r.t. theta1
            d = (P %*% dc)
            dTdtheta1 = -matrix(c((d[,1] * pk), (d[,2] * pk)), nrow=N, ncol=2)
            
            # browser()
            # Derivative of T w.r.t. theta3
            dtp = log(exp(-c + beta*EVnew) + exp(-RC - c[1] + beta*EVnew[1]))
            dTdtheta3 = cbind(
                c(dtp[1:(N-2)] - dtp[3:N], dtp[(N-1)] - dtp[N], 0),
                c(dtp[2:(N-1)] - dtp[3:N], 0, 0)
            )
            
            # Bind all parameter derivative vectors together
            dTdtheta = cbind(dTdb, dTdRC, dTdtheta1, dTdtheta3)
            # Derivative of EV w.r.t. theta
            dEV = solve(diag(N) - Tprime) %*% dTdtheta
            
            return(list(EVnew=EVnew, EVold=EVold, dEV=dEV))
        }
        EVold = EVnew
    }
    print(paste('MAX ITERATIONS REACHED! EV function approximated to', sup_norm))
    print(paste('Total seconds:', proc.time()[3] - start_t[3]))
    print(paste('Average seconds per iteration:', 
                (proc.time()[3] - start_t[3]) / iteration_max ))
    return(list(EVnew=EVnew, EVold=EVold))
}

theta1 = c(5, 0.01)
group_ = 1
theta3 = theta3.fun(group_ = group_, theta3_df)
beta = 0.99
RC = 10
x = 1:N
c = c2(x, theta1)
# Derivative of cost function w.r.t. theta1
dc = matrix(c(exp(theta1[2]*x), theta1[1]*x*exp(theta1[2]*x)), nrow=N, ncol=2)
# Generate the P matrix ()
P = P.fun(theta3)
EV_out = EV.sequential(theta1, theta3, c, RC, P, beta, verbose=F, accuracy_threshold = 1e-3)
pk = pk.fun(c, beta, EV_out$EVnew, RC)
Tprime = Tprime.fun(beta, P, pk)
EVnew = EV.nk(EV_out$EVnew, Tprime, c, dc, RC, P, beta, 
              verbose = T, accuracy_threshold = 1e-6)
# Choose the stepsize lambda
    # define D(theta) from (3.10)
    # define f(lambda; theta) = L(theta + lambda*D(theta))
    # define lambda_new from (3.13) using secant method
        # l2 = l1 - [(l1 - l0)*df(l1)/dl]/[df(l1)/dl - df(l0)/dl]
    # continue linesearch for lambda until (% inc in L) > (tolerance) and # iterations in (min_iter, max_iter)


# Recenter EV






```



evaluates the loglikelihood function `ll`, its derivatives `dll`,and the
information matrix (the cumulative outer product of the derivatives of the
individual loglikelihood terms). Consider first the evaluation of the
loglikelihood function. The general formula for the full and partial likelihood
function is given in equation (2.5) and (2.8) in chapter 2. The R code
(translated from GAUSS code) for these functions is given below:
```{r dLL}
# Calculating log-likelihood and derivative dLL

# Create T*M x 1 vector of P(0|x^m_t) for t in T and m in M (^1_1, ^1_2, ..., ^1_T, ^2_1, ...)
    # Take X bins from data, use to select indices of pk vector
lp = data %>%
    arrange(bus_number, m) %>%
    '$'(x5000) %>%
    pk[.]

# Create 2 x 1 vector of logs of parameters
```





The basic parameter iteration in BHHH given in equations (3.7), (3.10), and (3.13) have a
translation into GAUSS code given by
```{r BHHH}
# BHHH algorithm from (3.26)
theta_new = theta_old + labmda2*Dtheta  # Dtheta = D(theta_old)

Dtheta = solve(H) %*% dLL  # dLL = derivatives of log likelihood

lambda2 = lambda1 - (lambda1 - lambda0)*df1/(df1 - df0)  # dfk = dlambda_k/dlambda

df1 = t(dLL) %*% Dtheta
```




```{r old stuff, eval=F}


T.inner = function(x, y, i, ufun, EVfun, beta, xmax=N) {
    if (x+y > xmax) return(0)
    inner = exp(ufun(x+y, i) + beta*EVfun(x+y, i))
    # print(paste('x:', x,'  y:', y, '  i:', i, '  u:', ufun(x+y, i), '  EV:', EVfun(x+y, i)))
    return(inner)
}

T.inner_sum = function(x, y, ufun, EVfun, beta) {
    #' Return sum of T.inner over jvec
    inner_vec = sapply(c(0, 1), function(j) T.inner(x, y, j, ufun, EVfun, beta))
    # print('inner_vec:')
    # print(inner_vec)
    inner_sum = sum(inner_vec)
    return(inner_sum)
}

T.outer = function(x, y, ufun, EVfun, theta3, beta) {
    inner_sum = T.inner_sum(x, y, ufun, EVfun, beta)
    # print(paste('inner_sum:', inner_sum))
    log(inner_sum) * g.fun(y, theta3)
}

T.outer_sum = function(x, ufun, EVfun, theta3, beta) {
    yvec = comprehenr::to_vec(for (y in 0:2) if (x + y <= N) y)
    outer_vec = sapply(yvec, function(y) T.outer(x, y, ufun, EVfun, theta3, beta))
    outer_sum = sum(outer_vec)
}

EV2 = function(EVfun, cost_fun, theta1, RC, theta3, beta) {
    #' Return transformed EV function
    
    EVnew = function(x) {
        # sum over possible y
        yvec = comprehenr::to_vec(for (y in 0:2) if (x + y <= N) y)
        sum(sapply(yvec, function(y) {
                log(
                    exp(-cost_fun(x+y, theta1) + beta*EVfun(x+y)) +
                    exp(-RC - cost_fun(0, theta1) + beta*EVfun(0))
                ) * g.fun(y, theta3)
        }))
    }
    # EVnew = function(x, i) {
    #     ufun = U(x, i, cost_fun, theta1, RC)
    #     T.outer_sum(x, ufun, EVfun, theta3, beta)
    # }
    
    return(EVnew)
}



EVold = function(x, i) return(0)
theta1 = c(1, 1)
group_ = 1
theta3 = theta3.fun(group_ = group_, theta3_df)
beta = 0.99
RC = 1
cost_fun = c2
xvec = 1:N
for (k in 1:20) {
    start_t = proc.time()
    # print(paste('iteration', k, ' EVvec:'))
    # print(comprehenr::to_vec(for (x in xvec) for (i in 0:1) abs(EVold(x, i))))
    # Reduce old EV to vector function (instead of possibly many recursive functions)
    EVvec = comprehenr::to_vec(for (i in 0:1) for (x in xvec) EVold(x, i))
    EVold = function(x, i) return(EVvec[max(xvec)*i + x])
    # Get new EV
    EVnew = EV2(EVfun = EVold,
                cost_fun = cost_fun,
                theta1 = theta1,
                RC = RC,
                theta3 = theta3,
                beta = beta)
    # Calculate distance metric
    diff = comprehenr::to_vec(for (i in 0:1) for (x in xvec) abs(EVold(x, i) - EVnew(x, i)))
    sup_norm = max(diff)
    print(paste('iteration', k, ' norm:', sup_norm))
    Sys.sleep(5)
    if (sup_norm < 1e-6) {
        print('WITHIN TOLERANCE! EV function approximated.')
        break
    }
    EVold = EVnew
    print(paste('iteration seconds:', proc.time()[3] - start_t[3]))
}
```







\newpage
# Appendix A: R Code

```{r ref.label=knitr::all_labels(), echo=TRUE, eval=FALSE}
```





















